---
title: "Predicting Rocket League Game Outcome Through Statistical Modeling"
execute: 
  echo: false
  warning: false
format: 
  html:
    fig-height: 5
    fig-width: 8
    theme: quartz
    self-contained: true
---

```{r}
# Load in required packages and the datasets that will be used 
# for plots and modeling
library(tidyverse)
library(tidymodels)
library(tree)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(knitr)
library(here)
library(vip)
library(caret)
library(pander)
RL_joined <- read_csv(here("RL_Datasets/RL_joined.csv"))
RL_numeric <- read_csv(here("RL_Datasets/RL_numeric.csv"), 
        col_types = cols(winner_factor = readr::col_factor()))
# set themes for all future plots in this write up:
theme_set(theme_minimal(base_size = 12))
```

## Project Introduction:

  Over the past decade, the world of esports and competitive video-gaming has seen a drastic boom in popularity. One of the most popular esports is Rocket League, which is essentially soccer played in cars. 

  Rocket League is a video game that was released by Psyonix in 2015, and can be played on a PC, Xbox, PlayStation, and Nintendo Switch. Much like regular soccer, there is one ball and two goals, and a team wins by scoring more goals than the other team. The game is played with a randomly assigned “blue” and “orange” team, and each player controls one rocket-powered car. There are different game modes, but for the purposes of this project I will be focusing on the 3v3 game-mode, which is where the majority of competitive play occurs and is also the mode where most of the competitive cash prizes are. 
  
   Despite being released in 2015, the competitive and professional side of the video game is still growing to this day. In 2022, the total prize pool given out by Psyonix alone reached $6,000,000, and there are other non-Psyonix hosted tournaments with cash prizes as well. This past year, the video game has expanded to new regions, including Asia-Pacific North, Asia-Pacific South, and Middle East/North Africa. Players are constantly pushing the skill ceiling to re-define what it means to be the best in the world. With these improvements come constantly changing strategy and tactics that make for an even more thrilling viewer experience.

  Much like with popular sports such as baseball and football, there is a growing analytical side for assessing performance of some of the most popular esports. These are used to help strategize, coach, and improve competitive gamers’ performances. While analytics are useful to some competitive video games, Rocket League is a relatively new esport, and the statistical side of the game is widely unexplored. As both a fan and a player of Rocket League, and somebody with an interest in statistics and data science in general, I was curious to see if statistical learning algorithms could be applied to predicting game outcomes. These findings could potentially help form different strategies among professional teams and could even help coaches to decide what areas to put more of a focus on going into games.


## Introduction to Dataset/Variables:

  For this project, I only explored the three player vs. three player game-mode and looked at a dataset collected on the main professional competition that runs throughout the year; RLCS (Rocket League Championship Series). All of the data was obtained using octane.gg and ballchasing.com and spans over the course of multiple RLCS events throughout the 2021-2022 season. The dataset includes information on the teams in the match, team statistics (such as saves, assists, shots, etc.), boost statistics (boost used, boost stolen, time spent without boost, etc.), and movement statistics (time spent moving slow, time spent in each team’s half, time spent in air, etc.). Each game in the dataset had two rows: one for the statistics of the blue team and one for the statistics of the orange team.  
  
  Using this dataset, I calculated net variables by subtracting all of the blue team’s stats from all of the orange team’s stats. This allowed me to see the variables where having more or less than the opponent would be positively associated with winning. (For instance, examining whether a team’s chances of winning a match increased if they recorded more or less saves than their opponent.) I utilized a Random Forest model approach in order to try and predict whether teams would win/lose their match based on the net variables in the dataset.
  
  Additionally, I wanted to investigate whether or not different team play styles were consistently associated with winning/losing. To tackle this task, I used a dataset of individual player statistics on the same matches that I had used for team data. I wanted to see if teams who had certain players in different roles performed noticeably better/worse than teams who didn't play very positionally. (If one team had a "goalie" player, his saves would be noticeably higher than his teammates, a "goal-scorer" would have higher shots and goals than his teammates, etc.). To tackle this task, within each game for each team I calculated the standard deviation for all of the variables in the dataset and subsequently calculated the difference in these standard deviations for the blue team minus the orange team. 
  
As an illustrative example suppose that the players on the blue team recorded 4 shots, 3 shots, and 2 shots respectively. Also suppose that the players on the orange team recorded 6 shots, 5 shots, and 1 shot respectively. Then, the total shots would be 9 for blue and 12 for orange, so the *core_shots_diff* would be *-3* (always calculated as blue team - orange team to remain consistent). The standard deviation for the blue team would be roughly 0.82, and the standard deviation for the orange team would be roughly 2.16. Then, the *sd_core_shots_diff* would be 0.82 - 2.16 = *-1.34* for that game.

## Investigative Plots:

  To invesitgate the "diff" and "sd_diff" variables, I constructed side-by-side boxplots for each variable with whether or not the blue team won as the x-axis variable. There were some interesting observations that I made; one in particular that surprised me was that movement speed appears to have little to no correlation to match outcome. As a player myself, we are always coached that keeping your speed as high as possible around the field is vital in order to become a high level player. But, in this dataset of only professional matches, that does not appear to be the case.

**core_shots_diff plotted against winner:**
```{r}
ggplot(data = RL_joined, aes(x = winner, 
                             y = core_shots_diff)) + 
  geom_boxplot(fill="lightpink",colour ="black") +
  labs(y= "Shots Difference", x = "Winner")
```

  Two variables to note that provided some clear association to game outcome were “core_shots_diff” (difference in shots taken between the two teams), and “positioning_time_behind_ball_diff” (difference in the amount of time each team spent goal side of the ball). In general, the team that records a positive shot difference and the team that spends more time goalside of the ball tend to win games more often. In the plots below, TRUE corresponds to a game win, while FALSE corresponds to a loss.

**postioning_time_behind_ball_diff plotted against winner:**
```{r}
ggplot(data = RL_joined, aes(x = winner, 
                             y = positioning_time_behind_ball_diff)) + 
  geom_boxplot(fill="lightpink",colour ="black") +
  labs(y= "Positioning Time Behind Ball Difference", x = "Winner")
```

I expected a positive shot difference to correspond to winning, but I was surprised to find such a clear correlation between spending more time behind the ball being so strongly associated with winning games. 

## Methods:

As I mentioned previously, for this project I used a random forest statistical modeling approach. A random forest model is a type of statistical learning method that combines “decision trees” in order to make a prediction. Each tree is made up of a random subset of predictor variables from the full dataset. The predictions from each of these decision trees are combined to make one final prediction for each observation in the test data set. 

The random forest approach can be used for two main purposes: classification or regression. Since the goal of my model was to classify games as wins or losses, I used the classification approach.

My random forest model utilizes cross-validation. Cross-validation is a resampling method that creates different splits of training and test data to evaluate model performance. In this case, I performed 10-fold cross-validation so there were 10 different train and test sets that my model was analyzed on. One of the main benefits of cross-validation is that it helps to prevent the model from overfitting too closely to the data. Still, in case there was any chance that the model was yielding higher results because certain professional Rocket League series were partially in the training dataset and partially in the test dataset, I created a series_id variable by combining the names of the two teams in the series. When I did my cross-validation train/test splits, I grouped by series_id so that every series would either be entirely in the train set or entirely in the test set.

Once the data is split into groups of train and test data, the random forest approach utilizes boostrapping. Bootstrapping is essentially sampling the set-aside training data with replacement to create many different simulated samples. (Kaggle source) From there, a random subset of variables from the entire dataset (15 variables for each decision tree in my model), are used to create a decision tree that is fit to each of the bootstrapped samples of data. Each decision tree runs independently, and makes a classification prediction (predicted win or loss). (Kaggle source) Then, voting takes place, and whichever outcome was predicted by the highest number of decision trees is selected as the random forest model's final prediction. (edureka source) This process is visually depicted in the image below:

```{r}
knitr::include_graphics(here("Images/Random_Forest_Diagram.png"))
## source: https://www.analyticsvidhya.com/blog/2022/07/data-science-interview-series-part-2-random-forest-and-svm/
```

## Random Forest Results:

Utilizing the full dataset with all of the net difference variables as well as all of the standard deviation difference variables, I created a random forest model to predict the winner of each game. Using this random forest model that was trained on the training data, we then tested the model on the test data in order to analyze how strong the model was. The final step in this process was to get predictions on the game outcome of all the games in the test dataset, and see how accurately the model performed. The following gives the classification table for the model:


```{r}
# set a seed
set.seed(12)
RL_splits <- group_initial_split(RL_numeric, group = series_id, prop = 0.5)
RL_train <- training(RL_splits)
RL_test <- testing(RL_splits)
RL_resamples <- group_vfold_cv(RL_train, v = 5, group = series_id)


# Preprocess the data for modelling
RL_recipe <- recipe(winner_factor ~ ., data = RL_train) %>% 
  step_rm(series_id) %>%
  step_normalize(all_numeric_predictors()) 

# Build a random forest model specification
rf_spec <- rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

# Bundle recipe and model spec into a workflow
rf_wf <- workflow() %>% 
  add_recipe(RL_recipe) %>% 
  add_model(rf_spec)

# Fit a model
rf_wf_fit <- rf_wf %>% 
  fit(data = RL_train)

# Make predictions on test data
results <- RL_test %>% select(winner_factor) %>% 
  bind_cols(rf_wf_fit %>% 
              predict(new_data = RL_test)) %>% 
  bind_cols(rf_wf_fit %>% 
              predict(new_data = RL_test, type = "prob"))


# KABLE OR PANDER THIS 
# get results in a confusion matrix:
confusion_matrix <- results %>% 
  conf_mat(winner_factor, .pred_class)

# Convert confusion matrix to data frame
confusion_df <- as.data.frame(confusion_matrix$table)

# clean pander plot output of the confusion matrix
pander(confusion_df)

```

The model predicted 4615 true positives, 2850 true negatives, 764 false positives, and 510 false negatives. This combines for a total classification accuracy of 85.422%.

Below is an example of a single decision tree that utilizes the variables boost_time_zero_boost_diff (difference in the amount of time each team's players spent without any boost) and positioning_time_offensive_half_diff (the difference in the amount of time each team spent in the offensive half). In this example, the tree analyzes whether the difference in the time at zero boost is greater or less than 0.8. If it is greater than a 0.8 difference, it then looks to see if the difference in the amount of time spent in the offensive half between the two teams is more or less than 22 before making its prediction. (*It is important to pay attention to whether the node of the tree has a greater than or less than symbol.*)

```{r}
# create a dataset with a subset of variables to make an example tree
RL_numeric_tree_example <- RL_numeric |> 
                  select(c("winner_factor", 
                           "movement_time_high_air_diff",
                           "boost_time_zero_boost_diff",
                           "movement_time_slow_speed_diff",
                           "positioning_time_defensive_third_diff",
                           "positioning_time_offensive_half_diff"))

# fct_relevel to start factor variable at 0 instead of 1
RL_numeric_tree_example <- RL_numeric_tree_example |> 
  mutate(winner_factor = fct_relevel(winner_factor, "0"))

# plot the example decision tree
clean_tree <- rpart(winner_factor ~ ., 
                    data = RL_numeric_tree_example)
rpart.plot(clean_tree)


```

As an example following along with the decision tree above, let's say we are observing a Rocket League game where the blue team spent 3 more seconds with zero boost and 15 more seconds in the offensive half than the orange team. Then, the variable boost_time_zero_boost_diff = 3 and the variable positioning_time_offensive_half_diff = 15 for this game. Looking at the tree above, starting with observing boost_time_zero_boost_diff, 3 is greater than 0.8, so it goes down and to the left to examine the positioning_time_offensive_half_diff variable. The observed 15 second diffference is less than 22 seconds, so it moves down to the left to the bottom left bubble in the bottom row. At the top of that bubble the number is 0, which indicates a predicted loss for blue team. If the top of the bubble would have been a 1, that would have indicated a predicted win for the blue team.

A variable importance plot provides a way to analyze the significance of each of the predictors utilized in the random forest model. Gini impurity is a measurement used to build classification trees that tells the likelihood of misclassifying an observation within a node of a decision tree. Below is a variable importance plot that shows the 10 most important variables to my random forest model, based on mean decrease in Gini impurity:

```{r}
# Extract the fitted model from the workflow
vip_plot <- rf_wf_fit %>% 
  extract_fit_parsnip() 

# Make VIP plot
vip_plot <- vip(vip_plot, num_features = 10) +
  labs(color = "marital")

# plot 
vip_plot
```

In descending order, this plot demonstrates which variables the random forest model uses the most. The clear strongest variable in the model was the difference in teams' positioning time behind the ball. The difference in shots taken also proved to be a strong predictor in determining game outcome.

## Conclusion:

Through exploratory plots and multiple random forest models built, there were an unmistakable two predictors that were stronger than the rest. These variables were the difference in shots taken and the difference in time spent behind (or goalside) of the ball. It can be surprising at first that more time spent goal side of the ball provides such a strong chance of winning the game. But, from a strategical standpoint, it actually makes sense; one of the most common tactics in Rocket League that is also prominent in other sports is putting constant pressure on the opposing team. If a team is constantly putting the opposing team under pressure, the ball is going to constantly be in the opposing team's defensive half, and they probably will also be conceding a lot of shots. 

One of the most common lines players and coaches say is that teams should be making an effort to keep their car's speed/momentum high as much as possible throughout the course of the game. Interestingly, there were many variables related to movement speed, and almost none of them were significant predictors at all. The most significant movement variable toward predicting game outcome was movement_time_slow_speed_diff (the difference in the amount of time each team's cars spent moving slow), but even this was not a particularly strong predictor.

A potential explanation for this is that once matches reach a high enough level, every player is moving so fast the whole time and every player is capable of playing well while moving fast, that it becomes less of a strategic advantage than it is at some lower ranks in the game. It's entirely possible that for non-professional level players, this is still an extremely important factor, but once players reach a high enough skill level, it makes less of a difference.

Another interesting finding through this model was that almost none of the team standard deviation differences proved to be significant predictors. This means that through this model alone, we cannot determine whether different strategies are more or less effective to winning games, because there was no clear correlation. The fact that the data is all professional matches could be another reason why the standard deviations in variables did not prove to be important. At the highest level, players may already have all of their roles set and there may not be significant differences in tactical approaches among teams. However, it is possible that with further exploration into different team strategies that some may prove to be more effective. 








