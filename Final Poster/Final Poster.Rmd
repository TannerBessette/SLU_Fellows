---
title: "Predicting Rocket League Game Outcome Through Statistical Modeling"
author:
  - name: "By Tanner Bessette" 
  - name: "Faculty Advisor: Professor Matt Higham"
affiliation: 
  - address: "Department of Mathematics, Computer Science & Statistics"
  - address: "St. Lawrence University"
column_numbers: 3
logoright_name: https&#58;//raw.githubusercontent.com/TannerBessette/SLU_Fellows/main/Images/Rocket_League_Logo.png
logoleft_name: https&#58;//raw.githubusercontent.com/TannerBessette/SLU_Fellows/main/Images/St_Lawrence_Logo.png
output: 
  posterdown::posterdown_html:
    sections:
      - name: "What is Rocket League?"
        colspan: 1
      - name: "Project Goals"
        colspan: 1
        start: 2
      - name: "What is Random Forest?"
        colspan: 1
      - name: "Decision Tree Example"
        colspan: 1
        start: 2
      - name: "Results"
        colspan: 1
      - name: "Conclusion"
        colspan: 1
    self_contained: false
bibliography: packages.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# What is Rocket League?

-   One of the most popular video games/esports in the world

-   5 minute soccer games played in teams of 3 in rocket-powered cars

# Project Goals

-   Train a model on professional Rocket League games to predict match outcomes (win/loss) for Rocket League games

-   Assess which variables are most associated with winning games

-   Explain how certain tactics/strategies used by professional teams are more or less effective

# Dataset and Variables

-   Data spans multiple Rocket League Championship Series tournaments throughout 2021-2022

-   Team difference variables were created for each game by calculating blue team's stats minus orange team's stats

-   Example of row of data and some variables in the dataset:

    ```{r echo = FALSE, message = FALSE, out.width = "100%", out.height = "200%"}
    library(tidyverse)
    library(here)
    RL_numeric <- read_csv(here("RL_Datasets/RL_numeric.csv"), 
            col_types = cols(winner_factor = readr::col_factor()))
    # choose certain example variables to show
    RL_example_row <- RL_numeric %>%
                    select(winner_factor,
                        movement_time_supersonic_speed_diff,
                        positioning_time_defensive_half_diff,
                        core_shots_diff,
                        sd_core_shots_blue_diff,
                        boost_avg_amount_diff) 

    # rename sd_core_shots_blue_diff
    RL_example_row <- RL_example_row %>% 
      rename(sd_shots_diff = sd_core_shots_blue_diff,
             winner = winner_factor,
             supersonic_diff = movement_time_supersonic_speed_diff)

    # select only three variables to show from dataset:
    RL_example_row <- RL_example_row |>
      select(winner, supersonic_diff, sd_shots_diff)

    # pick a row from the dataset
    RL_example_row <- RL_example_row[2, ]

    # output an image of this row of data:

    RL_example_row |> knitr::kable()
    # image of it: knitr::include_graphics(here("Images/Dataset_row.png"))
    ```

* Variable descriptions:

    * `winner` is a `1` if blue team won, `0` if orange team won
    * `supersonic_diff` is the difference (blue minus orange) in total supersonic speed
    * `sd_shots_diff` is the difference (blue minus orange) in the standard deviation of shots taken across the three players on the team




# What is Random Forest?

1. A subset of the data, called the _training data_, is sampled with replacement in what is known as a _bootstrap sample_.

2. A random subset of variables are used to create a decision tree for the bootstrap sample.

3. The decision tree classifies each observation in a different subset of the data, known as the _test data_, as a win or loss. 

4. The process is repeated thousands of times: the result that was predicted by the highest number of decision trees is the model's final prediction for that observation.

```{r echo = FALSE, message = FALSE, out.width = "100%"}
library(here)
knitr::include_graphics(here("Images/Canva_RF.png"))
## source: https://www.analyticsvidhya.com/blog/2022/07/data-science-interview-series-part-2-random-forest-and-svm/'
```




# Decision Tree Example

```{r morefigs, out.width='90%', echo=FALSE, message = FALSE, fig.height = 4}
    #fig.cap='Amazing, right?!'
    library(tidyverse)
    library(tidymodels)
    library(tree)
    library(ggplot2)
    library(rpart)
    library(rpart.plot)
    library(knitr)
    library(here)
    library(vip)
    library(caret)
    # create a dataset with a subset of variables to make an example tree
    RL_numeric_tree_example <- RL_numeric |> 
                      select(c("winner_factor", 
                               "movement_time_high_air_diff",
                               "boost_time_zero_boost_diff",
                               "movement_time_slow_speed_diff",
                               "positioning_time_defensive_third_diff",
                               "positioning_time_offensive_half_diff"))

    # fct_relevel to start factor variable at 0 instead of 1
    RL_numeric_tree_example <- RL_numeric_tree_example |> 
      mutate(winner_factor = fct_relevel(winner_factor, "0"))

    # plot the example decision tree
    clean_tree <- rpart(winner_factor ~ ., 
                        data = RL_numeric_tree_example)
    rpart.plot(clean_tree)
```

-   Suppose there is a Rocket League game where `boost_time_zero_boost_diff` = `-1` and `positioning_time_offensive_half_diff` = `15`

-   In the first branch, `-1` is less than `0.8`, so the game is predicted to be a `1`, which corresponds to the blue team winning

- TANNER: THERE SHOULD NOW BE SPACE HERE TO REMIND READER THAT -1 CORRESPONDS TO ORANGE TEAM HAVING 1 MORE UNIT OF ZERO BOOST TIME THAN THE BLUE TEAM (no need to put this in the poster but just so you are aware: the cutoff point is not zero because the blue team won more often than the orange team in the data. we think they were the higher seeded team)

# Results

```{r out.width='80%', echo=FALSE, message = FALSE, fig.height = 4}

# set a seed
set.seed(12)
RL_splits <- group_initial_split(RL_numeric, group = series_id, prop = 0.5)
RL_train <- training(RL_splits)
RL_test <- testing(RL_splits)
RL_resamples <- group_vfold_cv(RL_train, v = 5, group = series_id)


# Preprocess the data for modelling
RL_recipe <- recipe(winner_factor ~ ., data = RL_train) %>% 
  step_rm(series_id) %>%
  step_normalize(all_numeric_predictors()) 

# Build a random forest model specification
rf_spec <- rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

# Bundle recipe and model spec into a workflow
rf_wf <- workflow() %>% 
  add_recipe(RL_recipe) %>% 
  add_model(rf_spec)

# Fit a model
rf_wf_fit <- rf_wf %>% 
  fit(data = RL_train)

# Extract the fitted model from the workflow
vip_plot <- rf_wf_fit %>% 
  extract_fit_parsnip() 

# Make VIP plot
vip_plot <- vip(vip_plot, num_features = 4) +
  labs(color = "marital") +
  theme_minimal()

# plot 
print(vip_plot)

# potentially add back to conclusion: , and players may already have all of their roles set within each team
```

TANNER: THERE SHOULD NOW BE ROOM TO EXPAND THE GRAPH IF YOU WANT TO TO SHOW MORE PREDICTORS.

-   Two clear strongest predictor variables: 

    - difference in positioning time behind the ball
    
    - difference in shots

-   None of the car speed predictors proved to be strong predictors

-   No standard deviation differences proved to be strong predictors

-   Final model predicted game outcome with **86.35% accuracy**

# Conclusion

-   Results reinforce the idea that keeping constant pressure on your opponent is the most effective Rocket League strategy

-   Keeping speed high throughout games may be effective in *reaching* the highest level, but once at the professional level it does not appear to be a significant indicator of game outcome

-   No significant standard deviation difference predictors means that we cannot definitively determine whether certain team tactics are more associated with winning games

    -   At the highest level of Rocket League, there may not be significant differences in tactical approaches among teams

    -   With further exploration, certain strategic approaches may still prove to be more effective
